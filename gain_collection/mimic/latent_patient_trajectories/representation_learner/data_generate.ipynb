{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "    \n",
    "def __getitem__(self, item):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        tensors (dict)\n",
    "            'rolling_ftseq', [batch_size, 119]\n",
    "            'ts', [batch_size, 239, 184]\n",
    "            'statics', [batch_size, 54]\n",
    "            'next_timepoint', [batch_size, 56]\n",
    "            'next_timepoint_was_measured', [batch_size, 56]\n",
    "            'disch_24h', [batch_size, 1]\n",
    "            'disch_48h', [batch_size, 1]\n",
    "            'Final Acuity Outcome', [batch_size, 1]\n",
    "            'ts_mask', [batch_size, 239]\n",
    "            'tasks_binary_multilabel', [batch_size, 7]\n",
    "            'note_ids', [batch_size, 239, 512]\n",
    "            'note_masks', [batch_size, 239, 512]\n",
    "            'note_segment_ids', [batch_size, 239, 512]\n",
    "            'note_hours_idx', [batch_size, 239]\n",
    "            'note_hours_num', [batch_size])\n",
    "    \"\"\"\n",
    "    # We'll use these for a bit of special processing surrounding our masked imputation task, so we\n",
    "    # define them now.\n",
    "    ts_vals_key, ts_is_measured_key, imputation_mask_key = 'ts_vals', 'ts_is_measured', 'ts_mask'\n",
    "    temptime = time.time()\n",
    "    # Loading data\n",
    "    try:\n",
    "        tensors = None\n",
    "        if self.epoch_cached:\n",
    "            tensors = self.cached_epoch[item]\n",
    "        else:\n",
    "            loaded, cached_item = self.load_save_path(item=item)\n",
    "            if loaded: tensors = cached_item\n",
    "        # print(\"gettime = \", time.time()-temptime)\n",
    "        return tensors\n",
    "        if tensors is not None:\n",
    "            if self.save_data_only: return {'null': torch.zeros((1, 1))}\n",
    "            tensors.update({k:v.float() for k,v in tensors.items() if v.dtype==torch.float16})\n",
    "\n",
    "            if 'rolling_fts' in tensors:\n",
    "                tensors['rolling_ftseq'] = tensors.pop('rolling_fts')\n",
    "\n",
    "            # Now adding the mask key.\n",
    "            if self.imputation_mask_rate > 0:\n",
    "                any_masked = False\n",
    "                while not any_masked:\n",
    "                    mask_prob = np.random.uniform(size=(self.max_seq_len, 1))\n",
    "                    any_masked = ((mask_prob < self.imputation_mask_rate).sum() > 0)\n",
    "                tensors[imputation_mask_key] = torch.Tensor(np.where(\n",
    "                    mask_prob < self.imputation_mask_rate,\n",
    "                    np.ones_like(mask_prob), np.zeros_like(mask_prob)\n",
    "                ))\n",
    "            elif 'ts_mask' in tensors: del tensors['ts_mask']\n",
    "            print(\"gettime = \", time.time()-temptime)\n",
    "            return tensors\n",
    "            # 若有cache就return了 否则会进入到后面的dfs\n",
    "    except:\n",
    "        print(f\"Failed to load item {item}\")\n",
    "        print(f\"Save path: {self.get_save_path(item=item)}\")\n",
    "        raise\n",
    "\n",
    "    # Now we actually need to create the item, but we may not have bothered to lad the dataframes yet. If\n",
    "    # not, we'll do that now.\n",
    "    try:\n",
    "        self.dfs\n",
    "        # 没有self.dfs\n",
    "        print_shapes = False\n",
    "    except AttributeError as e:\n",
    "        print(f\"Failed to load item from {self.get_save_path(item=item)}. Reloading dfs and creating it.\")\n",
    "        assert hasattr(self, 'reload_self_dir'), f\"Can't build items as lacks dfs or reload_self_dir!\"\n",
    "\n",
    "        full_self_path = os.path.join(self.reload_self_dir, f\"{self.train_tune_test}_dataset.pkl\")\n",
    "        assert os.path.isfile(full_self_path), f\"{full_self_path} doesn't exist! Can't reload dfs.\"\n",
    "\n",
    "        full_dataset = depickle(full_self_path)\n",
    "        self.dfs = full_dataset.dfs\n",
    "        self.subjects = full_dataset.subjects\n",
    "        self.orig_subjects = full_dataset.orig_subjects\n",
    "\n",
    "        self.reset_index()\n",
    "        print_shapes = True\n",
    "        print(\"Reloaded dfs. Continuing.\")\n",
    "\n",
    "    # Icustay id is always first.\n",
    "    idx = self.index[item]\n",
    "    if type(idx) is tuple:\n",
    "        icustay_id, end_time = idx\n",
    "        start_time = max(end_time - self.max_seq_len, 0)\n",
    "        seq_len = end_time - start_time\n",
    "    else:\n",
    "        icustay_id = idx\n",
    "        if self.sequence_len:\n",
    "            end_time   = self.sequence_len\n",
    "            start_time = max(end_time - self.max_seq_len, 0)\n",
    "            seq_len    = end_time - start_time\n",
    "        else:\n",
    "            max_seq_len = min(self.max_hours[item], self.max_seq_len)\n",
    "            end_time    = random.randint(self.min_seq_len, self.max_hours[item]) # the end time for this patient\n",
    "            start_time  = max(end_time - max_seq_len, 0) # the start time corresponding to the random_end_time\n",
    "            seq_len     = end_time - start_time\n",
    "\n",
    "    assert seq_len <= self.max_seq_len, f\"seq_len is {seq_len}, which is not less than or equal to max seq_length=={max_seq_len}\"\n",
    "\n",
    "    correction_attempts = 0\n",
    "    while 'rolling_fts' in self.dfs and 'rolling_ftseq' not in self.dfs:\n",
    "        try:\n",
    "            # print(\"Amending dfs to include rolling_ftseq\")\n",
    "            self.dfs['rolling_ftseq'] = self.dfs['rolling_fts']\n",
    "            self.dfs.pop('rolling_fts', None)\n",
    "        except: pass\n",
    "        correction_attempts += 1\n",
    "        if correction_attempts > 10:\n",
    "            raise ValueError(f\"Failed to correct dataframes fts v. ftseq bug!\")\n",
    "\n",
    "\n",
    "    # collect the indices for the patient\n",
    "    idxs = {k: (df.index.get_level_values('icustay_id') == icustay_id) for k, df in self.dfs.items()}\n",
    "    # We'll piggy back on our \"next_timepoint\" task for this imputation task. A more elegant solution\n",
    "    # would be to just store the measurement indicators and use them for both this task and the\n",
    "    # next timepoint prediction, but that's not how things are implemented for now.\n",
    "    idxs[ts_is_measured_key] = idxs['next_timepoint_was_measured'].copy()\n",
    "\n",
    "    # get the indices for each df between start_time and end_time\n",
    "    # Note for our special case of `ts_is_measured_key` & `next_timepoint_was_measured`, we still have it\n",
    "    # the case that the input features end at *<* end_time, whereas the target extractions are\n",
    "    # *==* end_time, so this should be valid.\n",
    "    for idxs_k, dfs_k in (\n",
    "        ('ts', 'ts'), ('notes', 'notes'), (ts_is_measured_key, 'next_timepoint_was_measured'),\n",
    "    ):\n",
    "        if idxs_k in idxs:\n",
    "            hours_in = self.dfs[dfs_k].index.get_level_values('hours_in')\n",
    "            idxs[idxs_k] &= ((hours_in >= start_time) & (hours_in < end_time))\n",
    "\n",
    "\n",
    "    # get the next task for predictions\n",
    "    for k in [\n",
    "        'rolling_tasks_binary_multilabel', 'rolling_tasks_multiclass', 'rolling_ftseq', 'next_timepoint',\n",
    "        'next_timepoint_was_measured',\n",
    "    ]:\n",
    "        if k not in self.dfs or self.dfs[k] is None: continue\n",
    "        if k in idxs: idxs[k] &= (self.dfs[k].index.get_level_values('hours_in') == end_time)\n",
    "\n",
    "    # get the correct subset of the dfs\n",
    "    dfs = {k: df.loc[idxs[k]].copy() for k, df in self.dfs.items() if df is not None}\n",
    "    dfs[ts_is_measured_key] = self.dfs['next_timepoint_was_measured'].loc[idxs[ts_is_measured_key]].copy()\n",
    "\n",
    "    # break up all of these dataframes that were processed as one into individual dfs\n",
    "    for k in ('rolling_tasks_multiclass', 'static_tasks_multiclass'):\n",
    "        df = dfs[k]\n",
    "        for c in df.columns:\n",
    "            dfs[c] = df[[c]]\n",
    "\n",
    "        del dfs[k]\n",
    "\n",
    "    if seq_len != len(dfs['ts']):\n",
    "        print(idx, start_time, end_time, self.sequence_len)\n",
    "        raise AssertionError(\"Length mismatch! %d v %d\" % (seq_len, len(dfs['ts'])))\n",
    "\n",
    "    # For the next timepoint, we only want the means of measured labs.\n",
    "    # TODO(mmd): Is this the right place for this logic? Or should it go earlier?\n",
    "    cols = dfs['next_timepoint'].columns\n",
    "    # print(cols)\n",
    "    mean_labs_cols = [c for c in cols if type(c) is tuple and c[1] == 'mean']\n",
    "    dfs['next_timepoint'] = dfs['next_timepoint'][mean_labs_cols].fillna(value=-1)\n",
    "\n",
    "    # Here, we pull out data for a masked imputation task. We want to store separately the continuous TS\n",
    "    # values (not imputed, as we don't want to predict imputed values), indicators of whether TS vals were\n",
    "    # measured (to mask out values we don't want to include in our imputation value and to predict what\n",
    "    # values should be imputed at any masked timepoint), and a mask key for the entire timeseries to\n",
    "    # indicate which timepoints are actually masked.\n",
    "\n",
    "    dfs[ts_vals_key] = dfs['ts'].loc[:, mean_labs_cols].copy().fillna(0)\n",
    "    # dfs[ts_is_measured_key] is already defined, based on the logic above.\n",
    "    # dfs[imputation_mask_key] we'll actually construct later, in the numpy arrays directly, as it doesn't have the\n",
    "    # same structure (e.g., column names) as the real dfs, we just need to match shape.\n",
    "\n",
    "\n",
    "    # TS continuous ais the only remaining actual timeseries feature.\n",
    "    # It needs to be imputed, padded, and reshaped.\n",
    "    dfs['ts'].loc[:, self.ts_continuous_cols] = self.impute_fn(\n",
    "        dfs['ts'].loc[:, self.ts_continuous_cols]\n",
    "    ).fillna(0) # First impute, then fill w/ 0.\n",
    "\n",
    "\n",
    "    if self.using_pretrained_notes:\n",
    "        dfs['notes'].loc[:, self.notes_cols] = self.impute_fn(\n",
    "            dfs['notes'].loc[:, self.notes_cols]\n",
    "        ).fillna(0) # First impute, then fill w/ 0.  # this is producing nans\n",
    "\n",
    "    np_arrays = {k: df.values for k, df in dfs.items()}\n",
    "    # We will deal with notes separately if we are integrating them instead of simply using pretrained embeddings\n",
    "    if not self.using_pretrained_notes:\n",
    "        np_arrays.pop('notes', None)\n",
    "\n",
    "    # Now adding the mask key.\n",
    "    if self.imputation_mask_rate > 0:\n",
    "        any_masked = False\n",
    "        while not any_masked:\n",
    "            mask_prob = np.random.uniform(size=(self.max_seq_len, 1))\n",
    "            any_masked = ((mask_prob < self.imputation_mask_rate).sum() > 0)\n",
    "        np_arrays[imputation_mask_key] = np.where(\n",
    "            mask_prob < self.imputation_mask_rate, np.ones_like(mask_prob), np.zeros_like(mask_prob)\n",
    "        )\n",
    "\n",
    "    # Padding\n",
    "    for k in ('ts', ts_vals_key, ts_is_measured_key, 'notes'):\n",
    "        if k in np_arrays:\n",
    "            num_features = np_arrays[k].shape[1]\n",
    "            if np_arrays[k].shape[0] != self.max_seq_len:\n",
    "                if self.max_seq_len > seq_len:\n",
    "                    pad = np.zeros((self.max_seq_len - seq_len, num_features))\n",
    "                    np_arrays[k] = np.expand_dims(np.concatenate((np_arrays[k], pad)), 0)\n",
    "            elif self.max_seq_len == seq_len:\n",
    "                np_arrays[k] = np.expand_dims(np_arrays[k], 0)\n",
    "\n",
    "    try:\n",
    "        np_arrays['tasks_binary_multilabel'] = np.concatenate(\n",
    "            [np_arrays[k] for k in self.binary_multilabel_task_concat_order], axis=1\n",
    "        )\n",
    "        del np_arrays['rolling_tasks_binary_multilabel']\n",
    "        del np_arrays['static_tasks_binary_multilabel']\n",
    "    except ValueError as e:\n",
    "        print(idx, start_time, end_time, self.sequence_len)\n",
    "        for k in self.binary_multilabel_task_concat_order:\n",
    "            print(f\"{k}: {np_arrays[k].shape}\")\n",
    "        raise\n",
    "\n",
    "    # Notes\n",
    "    if self.using_integrated_notes:\n",
    "        raise NotImplementedError(\"Doesn't support notes at present.\")\n",
    "\n",
    "    tensors = {}\n",
    "    for k, arr in np_arrays.items():\n",
    "        #assert arr.shape[0] == 1, f\"Must only have one first dimension for {k}! Got {arr.shape}\"\n",
    "        # print(k, arr.shape)\n",
    "        if arr.shape[0] == 1: tensors[k] = torch.tensor(arr[0])\n",
    "        else: tensors[k] = torch.tensor(arr)\n",
    "\n",
    "    if self.imputation_mask_rate == 0:\n",
    "        assert 'ts_mask' not in tensors, f\"{item}, {idx}, {k: t.shape for k, t in tensors.items()}\"\n",
    "    else:"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}